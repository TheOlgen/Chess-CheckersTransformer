import torch
import torch.nn as nn
import torch.nn.functional as F

class FullyConvTicTacToe(nn.Module):
    def __init__(self, input_channels=1, num_filters=32, num_conv_layers=3):
        super(FullyConvTicTacToe, self).__init__()
        
        layers = []
        in_channels = input_channels
        
        # Kilka warstw konwolucyjnych z ReLU
        for i in range(num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=3, padding=1))
            layers.append(nn.ReLU())
            in_channels = num_filters
        
        self.conv_layers = nn.Sequential(*layers)
        # Warstwa 1x1 - działa jak FC, ale na każdym polu osobno
        self.conv_head = nn.Conv2d(num_filters, 1, kernel_size=1)
        
    def forward(self, x):
        # x: [batch_size, 1, board_size, board_size]
        x = self.conv_layers(x)
        x = self.conv_head(x)  # [batch_size, 1, board_size, board_size]
        x = x.view(x.size(0), -1)  # spłaszczamy, by uzyskać [batch_size, board_size * board_size]
        return x

# Przykład użycia:
# Tworzymy model, nie zależny od stałego rozmiaru planszy:
model = FullyConvTicTacToe()

# Dla planszy 3x3:
inputs_3x3 = torch.randn(10, 1, 3, 3)
outputs_3x3 = model(inputs_3x3)  # kształt: [10, 9]

# Dla planszy 4x4:
inputs_4x4 = torch.randn(10, 1, 4, 4)
outputs_4x4 = model(inputs_4x4)  # kształt: [10, 16]

# Dla planszy 5x5:
inputs_5x5 = torch.randn(10, 1, 5, 5)
outputs_5x5 = model(inputs_5x5)  # kształt: [10, 25]